{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required modules\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "\n",
    "#Functions\n",
    "\"\"\"  \n",
    "### MANUAL ###\n",
    "get_rul_detail : get article address \n",
    "get_content : get article body(str type)\n",
    "JA_News : save article type:txt\n",
    "\"\"\"\n",
    "\n",
    "def get_url_detail(page_num, keyword=None):\n",
    "    keyword_html = parse.quote(keyword)\n",
    "    params1 = []\n",
    "    \n",
    "    for pagenum in range(1, page_num+1):\n",
    "        \n",
    "        #make url \n",
    "        p1 = \"https://news.joins.com/Search/TotalNews?Keyword=\"\n",
    "        p2 = keyword_html\n",
    "        p3 = \"&SortType=New&SearchCategoryType=TotalNews&PeriodType=All&ScopeType=All&ImageType=All&JplusType=All&BlogType=All&ImageSearchType=Image&TotalCount=0&StartCount=0&IsChosung=False&IssueCategoryType=All&IsDuplicate=True&Page=\"\n",
    "        p4 = str(pagenum)\n",
    "        p5 = \"&PageSize=10&IsNeedTotalCount=True\"\n",
    "        url_str = p1+p2+p3+p4+p5\n",
    "        \n",
    "        url = urllib.request.Request(url_str)\n",
    "        res = urllib.request.urlopen(url).read().decode('UTF-8')\n",
    "        stuff = BeautifulSoup(res, 'html.parser')\n",
    "        \n",
    "        content = stuff.find_all('h2', class_ = 'headline mg')\n",
    "            \n",
    "        for i in content:\n",
    "            params1.append(i.find_all('a')[0].get('href'))\n",
    "        \n",
    "    return params1\n",
    "\n",
    "\n",
    "def get_content(page_num, keyword):\n",
    "    url_str = get_url_detail(page_num, keyword=keyword)\n",
    "    params2 = []\n",
    "    \n",
    "    for i in url_str:\n",
    "        time.sleep(1)    #Recommand 1~5 seconds\n",
    "        url = urllib.request.Request(i)\n",
    "        res = urllib.request.urlopen(url).read().decode('utf8')\n",
    "        stuff = BeautifulSoup(res, 'html.parser')\n",
    "        \n",
    "        content = stuff.find_all('div', class_ = 'article_body mg fs4')\n",
    "        \n",
    "        for i in content:\n",
    "            params2.append(i.get_text('\\n',strip = True))\n",
    "            \n",
    "    return params2\n",
    "\n",
    "\n",
    "def JA_News(keyword='None', save_path=\"C:\", file_name=\"sample\", page_num=1):\n",
    "    article = get_content(page_num, keyword)\n",
    "    with open(save_path + '\\\\' + \"JA_\" + file_name + '.txt', 'w', encoding='utf8') as f:\n",
    "        for idx, x in enumerate(article, 1):\n",
    "            f.write('\\n\\n#'+str(idx)+'\\n'+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JA_News(keyword=\"교통사고\", save_path=\"C:\\\\Users\\\\82103\\\\OneDrive\\\\#Project_WordCLoud\", file_name=\"sample002\", page_num=1)  #한글검색1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
