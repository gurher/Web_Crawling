{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Donga.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Nxau1bGFzk"
      },
      "source": [
        "\n",
        "#Import required modules\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib import parse\n",
        "import time\n",
        "\n",
        "\n",
        "def get_url_detail(page_num, keyword=None):\n",
        "  params = []\n",
        "\n",
        "  for page in range(1,page_num*15, 15):\n",
        "    p1 = 'https://www.donga.com/news/search?p='\n",
        "    p2 = str(page)\n",
        "    p3 =  parse.quote(search)\n",
        "    p4 = '&query=%EA%B5%90%ED%86%B5%EC%82%AC%EA%B3%A0&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'    \n",
        "\n",
        "    url_str = p1 + p2 + p3 + p4\n",
        "    url = urllib.request.Request(url_str)\n",
        "    res = urllib.request.urlopen(url).read().decode('utf8')\n",
        "    stuff = BeautifulSoup(res, 'html.parser')\n",
        "    content = stuff.find_all('p', class_ = \"txt\")\n",
        "    \n",
        "    for i in content:\n",
        "      params.append(i.find_all('a')[0].get('href'))\n",
        "\n",
        "  return params\n",
        "\n",
        "def get_content(page_num, keyword):\n",
        "    url_str = get_url_detail(page_num, keyword=keyword)\n",
        "    params1 = []\n",
        "    \n",
        "    for i in url_str:\n",
        "        time.sleep(1)    \n",
        "        url = urllib.request.Request(i)\n",
        "        res = urllib.request.urlopen(url).read().decode('utf8')\n",
        "        stuff = BeautifulSoup(res, 'html.parser')\n",
        "        \n",
        "        content = stuff.find_all('div',class_ ='article_txt')\n",
        "        \n",
        "        for i in content:\n",
        "            params1.append(i.get_text(\" \",strip=True))\n",
        "            \n",
        "    return params1\n",
        "\n",
        "def Donga_News(keyword='None', save_path=\"c:\", file_name=\"Donga\", page_num=1):\n",
        "    article = get_content(page_num, keyword)\n",
        "    with open(save_path + '\\\\' + file_name + '.txt', 'w', encoding='utf8') as f:\n",
        "        for idx, x in enumerate(article, 1):\n",
        "            f.write('\\n\\n#'+str(idx)+'\\n'+x)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTKWGLJ9GwK-"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjPaS4usGN8_"
      },
      "source": [
        "Donga_News('교통사고',page_num=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}